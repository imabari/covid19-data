{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ehime_covid_tsv.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNUe/uSAtVp4IGMVG1i+jY5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/imabari/covid19-data/blob/master/ehime/ehime_covid_tsv.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8aTNI9aRhe7",
        "outputId": "c4bf9180-a1ad-4da7-d207-564c7ce38c6f"
      },
      "source": [
        "!pip install pdfplumber"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pdfplumber\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/57/4d9768e9ed204c68bd5813a2a112d3d6af4912f0785d47080b5067cdce64/pdfplumber-0.5.27.tar.gz (44kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 2.7MB/s \n",
            "\u001b[?25hCollecting pdfminer.six==20200517\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/c0/ef1c8758bbd86edb10b5443700aac97d0ba27a9ca2e7696db8cd1fdbd5a8/pdfminer.six-20200517-py3-none-any.whl (5.6MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6MB 5.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: Pillow>=7.0.0 in /usr/local/lib/python3.7/dist-packages (from pdfplumber) (7.1.2)\n",
            "Collecting Wand\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/f6/05f043c099639b9017b7244791048a4d146dfea45b41a199aed373246d50/Wand-0.6.6-py2.py3-none-any.whl (138kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 49.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet; python_version > \"3.0\" in /usr/local/lib/python3.7/dist-packages (from pdfminer.six==20200517->pdfplumber) (3.0.4)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from pdfminer.six==20200517->pdfplumber) (2.3.0)\n",
            "Collecting pycryptodome\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/16/9627ab0493894a11c68e46000dbcc82f578c8ff06bc2980dcd016aea9bd3/pycryptodome-3.10.1-cp35-abi3-manylinux2010_x86_64.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 52.1MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pdfplumber\n",
            "  Building wheel for pdfplumber (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pdfplumber: filename=pdfplumber-0.5.27-cp37-none-any.whl size=32071 sha256=0dedeba7bebff1e8f4bc3e1a6bbb043fde98447b6189c5a712de8b33c716fcc5\n",
            "  Stored in directory: /root/.cache/pip/wheels/b4/74/fc/f7b3a1a0732209027fb48a5f4392fc40d79970b11c2ba49e71\n",
            "Successfully built pdfplumber\n",
            "Installing collected packages: pycryptodome, pdfminer.six, Wand, pdfplumber\n",
            "Successfully installed Wand-0.6.6 pdfminer.six-20200517 pdfplumber-0.5.27 pycryptodome-3.10.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LYLByHJww4s"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhIRIw1Gww4t"
      },
      "source": [
        "from urllib.parse import urljoin\n",
        "import pathlib"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdpxH5CwRhe9"
      },
      "source": [
        "import pdfplumber\n",
        "import pandas as pd"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "virGw9bu83AZ"
      },
      "source": [
        "# スクレイピング"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AE0btIqww4t"
      },
      "source": [
        "url = \"https://www.pref.ehime.jp/h25500/kansen/covid19.html\"\n",
        "# url = \"https://www.pref.ehime.jp/h25500/kansen/covid19/kansensya-kako.html\"\n",
        "\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; rv:11.0) like Gecko\"\n",
        "}"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9L0rc2Yfww4t"
      },
      "source": [
        "r = requests.get(url, headers=headers)\n",
        "r.raise_for_status()\n",
        "\n",
        "soup = BeautifulSoup(r.content, \"html.parser\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7Uf7MFYxPmI"
      },
      "source": [
        "tags = [i for i in soup.select(\"div#tmp_contents > ul > li > a\") if \"新型コロナウイルスの感染の確認等について\" in i.get_text(strip=True)]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5DiZG-p8wqj"
      },
      "source": [
        "# ダウンロード"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peenu36OyXgt"
      },
      "source": [
        "def fetch_file(url, dir=\".\"):\n",
        "\n",
        "    p = pathlib.Path(dir, pathlib.PurePath(url).name)\n",
        "    p.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    if p.exists():\n",
        "        print(f\"{p}\\t同一のファイルが存在するためダウンロードを中止します\")\n",
        "\n",
        "    else:\n",
        "        r = requests.get(url)\n",
        "        r.raise_for_status()\n",
        "\n",
        "        with p.open(mode=\"wb\") as fw:\n",
        "            fw.write(r.content)\n",
        "    return p"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85BoJe1rDyZ3"
      },
      "source": [
        "def find_cluster(data):\n",
        "\n",
        "    for d in data:\n",
        "\n",
        "        s = d.get(\"text\", \"\")\n",
        "\n",
        "        if s.startswith(\"○クラスターの状況\"):\n",
        "\n",
        "            return d.get(\"top\")\n",
        "    \n",
        "    return None"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7-8Azaoa2BJ"
      },
      "source": [
        "def data_conv(ser: pd.Series, col: str) -> pd.Series:\n",
        "\n",
        "    df = ser.str.split(\"\\n+\", expand=True).T[0].str.split(\"：\", expand=True).rename(columns={0: col, 1: \"人数\"})\n",
        "\n",
        "    df[col] = df[col].str.strip().str.normalize(\"NFKC\").str.replace(\"\\s\", \"\", regex=True)\n",
        "\n",
        "    df[\"人数\"] = df[\"人数\"].str.strip().str.rstrip(\"名\").str.normalize(\"NFKC\").astype(int)\n",
        "\n",
        "    return df.set_index(col)[\"人数\"]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YaxHjgD1LvPU"
      },
      "source": [
        "dfs_ages = []\n",
        "dfs_area = []\n",
        "dfs_sex = []\n",
        "\n",
        "for tag in tags:\n",
        "\n",
        "    link = urljoin(url, tag.get(\"href\"))\n",
        "    p = fetch_file(link, \"download\")\n",
        "\n",
        "    name = p.stem\n",
        "\n",
        "    pdf = pdfplumber.open(p)\n",
        "    page = pdf.pages[0]\n",
        "\n",
        "    top = find_cluster(page.extract_words())\n",
        "    hight = top or page.height\n",
        "    crop = page.within_bbox((0, 80, page.width, hight))\n",
        " \n",
        "    tables = sorted(crop.find_tables(), key=lambda t: t.bbox)\n",
        "    table = tables[0].extract()\n",
        "\n",
        "    tmp = pd.DataFrame(table[1:], columns=table[0])\n",
        "\n",
        "    # 年代\n",
        "    tmp_ages = data_conv(tmp[\"年代\"], \"年代\")\n",
        "\n",
        "    # 居住地\n",
        "    tmp_area = data_conv(tmp[\"居住地\"], \"居住地\")\n",
        "\n",
        "    # 性別\n",
        "    tmp_sex = tmp[\"性別\"].str.extractall(\"([男|女]性)：(.+)名\").rename(columns={0: \"性別\", 1: \"人数\"}).set_index(\"性別\").astype(int)[\"人数\"]\n",
        "\n",
        "    tmp_ages.name = name\n",
        "    tmp_area.name = name\n",
        "    tmp_sex.name = name\n",
        "\n",
        "    dfs_ages.append(tmp_ages)\n",
        "    dfs_area.append(tmp_area)\n",
        "    dfs_sex.append(tmp_sex)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPGzzH0NVqD8"
      },
      "source": [
        "df_ages = pd.concat(dfs_ages, axis=1).T.fillna(0).astype(int).sort_index().reindex(columns = [\"10歳未満\", \"10代\", \"20代\", \"30代\", \"40代\", \"50代\", \"60代\", \"70代\", \"80代\", \"90代\"])\n",
        "df_area = pd.concat(dfs_area, axis=1).T.fillna(0).astype(int).sort_index()\n",
        "df_sex = pd.concat(dfs_sex, axis=1).T.fillna(0).astype(int).sort_index()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JutHfNQaYY_B"
      },
      "source": [
        "df_ages.to_csv(\"ages.tsv\", sep=\"\\t\")\n",
        "df_area.to_csv(\"area.tsv\", sep=\"\\t\")\n",
        "df_sex.to_csv(\"sex.tsv\", sep=\"\\t\")"
      ],
      "execution_count": 13,
      "outputs": []
    }
  ]
}